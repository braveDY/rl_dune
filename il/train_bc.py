"""
è¡Œä¸ºå…‹éš†ï¼ˆBehavior Cloningï¼‰æ¨¡ä»¿å­¦ä¹ 
ä½¿ç”¨ä¸“å®¶æ•°æ®è®­ç»ƒç­–ç•¥ç½‘ç»œ
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pickle
from pathlib import Path
from tqdm import tqdm


class ExpertDataset(Dataset):
    """ä¸“å®¶æ•°æ®é›†"""
    def __init__(self, data_path):
        with open(data_path, 'rb') as f:
            data = pickle.load(f)
        
        self.observations = torch.FloatTensor(data['observations'])
        self.actions = torch.FloatTensor(data['actions'])
        print(f"åŠ è½½æ•°æ®: {len(self.observations)} æ¡transitions")
    
    def __len__(self):
        return len(self.observations)
    
    def __getitem__(self, idx):
        return self.observations[idx], self.actions[idx]


class BCPolicy(nn.Module):
    """è¡Œä¸ºå…‹éš†ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, obs_dim, action_dim, action_low, action_high, hidden_dim=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()  # è¾“å‡º[-1, 1]èŒƒå›´
        )
        # åŠ¨ä½œç©ºé—´ç¼©æ”¾å‚æ•°
        self.register_buffer('action_low', torch.FloatTensor(action_low))
        self.register_buffer('action_high', torch.FloatTensor(action_high))
        self.register_buffer('action_scale', (self.action_high - self.action_low) / 2.0)
        self.register_buffer('action_bias', (self.action_high + self.action_low) / 2.0)
    
    def forward(self, obs):
        # è¾“å‡º[-1, 1]ï¼Œç„¶åç¼©æ”¾åˆ°å®é™…åŠ¨ä½œç©ºé—´
        action = self.net(obs)
        return action * self.action_scale + self.action_bias


def train_bc(data_path, save_path, action_low, action_high, epochs=10000, batch_size=256, lr=1e-3):
    """è®­ç»ƒBCç­–ç•¥"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åŠ è½½æ•°æ®
    dataset = ExpertDataset(data_path)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # æ•°æ®è¯Šæ–­
    print(f"\nğŸ“Š æ•°æ®è¯Šæ–­:")
    print(f"è§‚æµ‹å½¢çŠ¶: {dataset.observations.shape}")
    print(f"åŠ¨ä½œå½¢çŠ¶: {dataset.actions.shape}")
    print(f"è§‚æµ‹èŒƒå›´: [{dataset.observations.min():.2f}, {dataset.observations.max():.2f}]")
    print(f"åŠ¨ä½œèŒƒå›´: [{dataset.actions.min():.2f}, {dataset.actions.max():.2f}]")
    print(f"åŠ¨ä½œå‡å€¼: {dataset.actions.mean(dim=0)}")
    print(f"åŠ¨ä½œæ ‡å‡†å·®: {dataset.actions.std(dim=0)}")
    print(f"æœŸæœ›åŠ¨ä½œèŒƒå›´: [{action_low[0]:.2f}, {action_low[1]:.2f}] åˆ° [{action_high[0]:.2f}, {action_high[1]:.2f}]")
    
    # åˆ›å»ºæ¨¡å‹
    obs_dim = dataset.observations.shape[1]
    action_dim = dataset.actions.shape[1]
    policy = BCPolicy(obs_dim, action_dim, action_low, action_high).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=lr)
    criterion = nn.MSELoss()
    
    print(f"\nå¼€å§‹è®­ç»ƒBCç­–ç•¥...")
    print(f"è§‚æµ‹ç»´åº¦: {obs_dim}, åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"è®¾å¤‡: {device}")
    
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    pbar = tqdm(range(epochs), desc="è®­ç»ƒBCç­–ç•¥", ncols=80)
    for epoch in pbar:
        total_loss = 0
        num_batches = 0
        
        for obs, actions in dataloader:
            obs, actions = obs.to(device), actions.to(device)
            
            # å‰å‘ä¼ æ’­
            pred_actions = policy(obs)
            loss = criterion(pred_actions, actions)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
                
        pbar.set_description(f"Loss: {total_loss:.3f}")
            
        # æ˜¾ç¤ºé¢„æµ‹æ ·æœ¬
        if (epoch + 1) % 100 == 0:
            with torch.no_grad():
                sample_obs = dataset.observations[:5].to(device)
                sample_actions = dataset.actions[:5]
                pred = policy(sample_obs).cpu()
                print(f"  çœŸå®åŠ¨ä½œ: {sample_actions[0].numpy()}")
                print(f"  é¢„æµ‹åŠ¨ä½œ: {pred[0].numpy()}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if total_loss < best_loss:
            best_loss = total_loss
            torch.save(policy.state_dict(), save_path.replace('.pth', '_best.pth'))
        
    
    # ä¿å­˜æ¨¡å‹
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(policy.state_dict(), save_path)
    print(f"\nâœ“ BCç­–ç•¥å·²ä¿å­˜åˆ°: {save_path}")
    
    return policy


if __name__ == "__main__":
    from environment.env_dune import V_MIN, V_MAX, W_MIN, W_MAX
    
    train_bc(
        data_path="imitation_data/expert_demonstrations.pkl",
        save_path="models/bc_policy.pth",
        action_low=[V_MIN, W_MIN],
        action_high=[V_MAX, W_MAX],
        epochs=50000,  # å…ˆç”¨å°‘é‡epochæµ‹è¯•
        batch_size=256,
        lr=1e-3
    )